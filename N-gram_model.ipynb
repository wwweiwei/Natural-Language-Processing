{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab_1_h092546.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4-J5jkcatiQ"
      },
      "source": [
        "**Lab1 Reminder: **\n",
        "1. Use small data to develop your code.\n",
        "2. Relink to the demomstration code can check the output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqVxwKpgCe0a",
        "outputId": "57ad4c87-431e-4f7f-8579-5c7d71d4b1d6"
      },
      "source": [
        "# For debugging\n",
        "import pdb\n",
        "\n",
        "# For checking progress\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For loading data\n",
        "import pandas as pd\n",
        "\n",
        "# For tokenizaton\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# For building n-gram model\n",
        "from collections import Counter, namedtuple\n",
        "import numpy as np\n",
        "\n",
        "# For pos tagging\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4r6vYRwwV6m"
      },
      "source": [
        "# Part 1. Data Preprocessing\n",
        "1. show the top-10 common words and their counts before/after preprocessing\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXl7m78Oxz9d"
      },
      "source": [
        "## Functions and Classes\n",
        "*  Remove the punctuations\n",
        "*  Lower the cases\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy2bDYorrKxW"
      },
      "source": [
        "def get_corpus():\n",
        "  \"\"\" Reads and formats the corpus.\n",
        "\n",
        "  Returns:\n",
        "    corpus (list[str]):\n",
        "      A list of sentences in the corpus.\n",
        "  \"\"\"\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/yunzhusong/NLP109/main/lab1_data.csv')\n",
        "  corpus = df.content.to_list()\n",
        "  return corpus"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqW8SUq_xzCm"
      },
      "source": [
        "def preprocess(documents):\n",
        "  \"\"\" Preprocesses the corpus.\n",
        "  \n",
        "  Args:\n",
        "    documents (list[str]):\n",
        "      A list of sentences in the corpus.\n",
        "  Returns:\n",
        "    cleaned_documents (list[str]):\n",
        "      A list of cleaned sentences in the corpus.\n",
        "  \"\"\"\n",
        "  cleaned_documents = []\n",
        "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~”'''\n",
        "  for doc in documents:\n",
        "    # Tokenizes the sentence\n",
        "    sents = sent_tokenize(doc)\n",
        "\n",
        "    for sent in sents:\n",
        "      #pdb.set_trace() # delete this line for the final version\n",
        "\n",
        "      # Removes the punctuations, hint: recursively remove in character level\n",
        "      # Lowers the case\n",
        "\n",
        "      ## [TODO]\n",
        "      new_sent = ''\n",
        "      for word in sent:\n",
        "        if word not in punc: \n",
        "          new_sent = new_sent + word.lower()\n",
        "        else: new_sent = new_sent + ' '\n",
        "    \n",
        "      cleaned_documents.append(new_sent)\n",
        "\n",
        "  print(cleaned_documents[:5])\n",
        "  return cleaned_documents"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLxv8uXpxvC2"
      },
      "source": [
        "# Compute word frequency\n",
        "def get_vocab(documents):\n",
        "  \"\"\" Gets the vocabulary from the corpus.\n",
        "  \n",
        "  Args:\n",
        "    documents (list[str]):\n",
        "      A list of sentences in the corpus\n",
        "  Returns:\n",
        "    vocabulary (collections.Counter)\n",
        "  \"\"\"\n",
        "  vocabulary = Counter()\n",
        "\n",
        "  for doc in tqdm(documents):\n",
        "    tokens = word_tokenize(doc)\n",
        "    vocabulary.update(tokens)\n",
        "\n",
        "  return vocabulary"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Au2w1vJjHqP"
      },
      "source": [
        "## Executions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkCSSWp3j65G"
      },
      "source": [
        "### 1. Show the top-10 common words and their counts before/after preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEDgtK3RxFXt",
        "outputId": "da4ea6d4-c029-4599-de3d-3ee2aa141514"
      },
      "source": [
        "# Read data\n",
        "raw_documents = get_corpus()\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = get_vocab(raw_documents).most_common(10)\n",
        "print('\\n Before preprocessing:', vocab)\n",
        "\n",
        "# Build vocabulary after preprocessing\n",
        "documents = preprocess(raw_documents)\n",
        "vocab = get_vocab(documents).most_common(10)\n",
        "print('\\n After preprocesing:', vocab)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100000/100000 [00:23<00:00, 4236.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " Before preprocessing: [('.', 85947), ('the', 49772), (',', 39728), ('to', 34407), ('!', 33580), ('a', 28765), ('is', 26339), ('?', 24057), ('and', 22890), ('of', 22542)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  1%|          | 882/175323 [00:00<00:19, 8817.99it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "['the president is unhinged', 'shucks  they made acosta cry', 'this is what divides us and caused the hate', 'keep telling yourself those lies and bot sentinel is set up by people like you for anyone who doesn t agree with all your views  well guess what kiddo   chances are in life there are going to be a lot of people who don t agree with you ', 'already debunked last weekend ']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 175323/175323 [00:20<00:00, 8390.57it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " After preprocesing: [('the', 56446), ('to', 35130), ('a', 30363), ('you', 29262), ('is', 27200), ('and', 25790), ('of', 23117), ('it', 19647), ('i', 18268), ('that', 17711)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_N2g0CIcB-_"
      },
      "source": [
        "- **Before preprocessing:** [('.', 85947), ('the', 49772), (',', 39728), ('to', 34407), ('!', 33580), ('a', 28765), ('is', 26339), ('?', 24057), ('and', 22890), ('of', 22542)]\n",
        "- **After preprocesing:** [('the', 56207), ('to', 35063), ('a', 30127), ('is', 27100), ('you', 27077), ('and', 25543), ('of', 23077), ('that', 16069), ('it', 15965), ('in', 15898)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-mCeiF3inh-"
      },
      "source": [
        "# Part 2. N-Gram Model and POS Tagging\n",
        "1. Build 2-gram / 4-gram model by processed dataset\n",
        "2. Show the top-5 probable next words and their probability after initial token ‘\\<s\\>’ by 2-gram model\n",
        "3. Generate a sentence with 2-gram model and find the POS taggings\n",
        "4. Generate a sentence with 4-gram model and find the POS taggings\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7MRk6EMjlKL"
      },
      "source": [
        "## Functions and Classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsStQG516jC6"
      },
      "source": [
        "class Ngram_model(object):\n",
        "  \"\"\" Ngram model implementation.\n",
        "\n",
        "  Attributes:\n",
        "    n (int):\n",
        "      The number of grams to be considered.\n",
        "    model (dict):\n",
        "      The ngram model.\n",
        "  \"\"\"\n",
        "  def __init__(self, documents, N=2):\n",
        "    self.n = N\n",
        "    self.model = self.get_ngram_model(documents)\n",
        "\n",
        "  def get_ngram_model(self, documents):\n",
        "    N = self.n\n",
        "    ngram_model = dict()\n",
        "    full_grams = list()\n",
        "    grams = list()\n",
        "    Word = namedtuple('Word', ['word', 'prob'])\n",
        "\n",
        "    # for each sentence in documents\n",
        "    ##[TODO]\n",
        "    for sent in documents:\n",
        "      \n",
        "      # Tokenizes to words\n",
        "      # Append (N-1) start tokens '<s>' and an end token '<\\s>'\n",
        "      ## [TODO] \n",
        "      tokens = []\n",
        "      for i in range(N-1):\n",
        "        tokens = tokens + ['<s>']\n",
        "      tokens = tokens + nltk.word_tokenize(sent)+ ['<\\s>']\n",
        "\n",
        "      # Calculates numerator (construct list with full grams, i.e., N-grams)\n",
        "      ##[TODO]    \n",
        "      for i in range(len(tokens)-N+1):\n",
        "        gram = []\n",
        "        for j in range(N):\n",
        "          gram.append(tokens[i+j])\n",
        "        full_grams.append(tuple(gram))\n",
        "\n",
        "      # Calculate denominator (construct list with grams, i.e., (N-1)-grams)\n",
        "      ##[TODO]\n",
        "      for i in range(len(tokens)-N+2):\n",
        "        gram = []\n",
        "        for j in range(N-1):\n",
        "          gram.append(tokens[i+j])\n",
        "        grams.append(tuple(gram))\n",
        "      \n",
        "    # Count the occurence frequency of each gram\n",
        "    # Take 2-gram model as example:\n",
        "    #   full_grams -> list[('a', 'gram'),('other', 'gram'), ...]\n",
        "    #   grams -> list[('a'), ('other'), ('gram'), ...]\n",
        "    #   full_gram_counter -> dict{('a', 'gram'):frequency_1, ('other','gram'):frequency_2, ...}\n",
        "    #   gram_counter -> dict{('a'):frequency_1, ('gram'):frequency_2, ...}\n",
        "    full_gram_counter = Counter(full_grams)\n",
        "    gram_counter = Counter(grams)\n",
        "\n",
        "    # Build model\n",
        "    # Take 2-gram model as example:\n",
        "    #   { '<s>': [tuple(word='i', prob=0.6), tuple(word='the', prob=0.2), ...],\n",
        "    #   'i': [tuple(word='am', prob=0.7), tuple(word='want', prob=0.1), ...],\n",
        "    #    ... }\n",
        "    for key in full_gram_counter:\n",
        "      word = ''.join(key[:N-1])\n",
        "\n",
        "      if word not in ngram_model:\n",
        "        ngram_model.update({word: set()})\n",
        "\n",
        "      # next_word_prob -> float\n",
        "      next_word_prob = full_gram_counter[key] / gram_counter[key[:N-1]]\n",
        "      w = Word(key[-1], next_word_prob)\n",
        "      ngram_model[word].add(w)\n",
        "\n",
        "    # Sort the result by frequency\n",
        "    for word, ng in ngram_model.items():\n",
        "      ngram_model[word] = sorted(ng, key=lambda x: x.prob, reverse=True)\n",
        "\n",
        "    return ngram_model\n",
        "\n",
        "\n",
        "  def predict_sent(self, text=None, max_len=30):\n",
        "    \"\"\" Predicts a sentence with the ngram model.\n",
        "\n",
        "    Args:\n",
        "      text (string or list[string])\n",
        "    Returns:\n",
        "      A prediction string.\n",
        "    \"\"\"\n",
        "\n",
        "    N = self.n\n",
        "    backup_tokens = ['<s>']*(N-1)\n",
        "    if not text:\n",
        "      tokens = backup_tokens\n",
        "      output = []\n",
        "\n",
        "    elif type(text)==str:\n",
        "      tokens = backup_tokens + text.split(' ')\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return \n",
        "      output = tokens\n",
        "\n",
        "    elif type(text) == list:\n",
        "      tokens = backup_tokens + text\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return\n",
        "      output = tokens\n",
        "\n",
        "    else:\n",
        "      print('[Error] the input text must be string or list of string')\n",
        "      return\n",
        "\n",
        "    for i in range(max_len):\n",
        "      possible_words = list(self.model[''.join(tokens)])\n",
        "      probs = [word.prob for word in possible_words]\n",
        "      words = [word.word for word in possible_words]\n",
        "      next_word = np.random.choice(words, 1, p=probs)[0]\n",
        "      tokens = tokens[1:] + [next_word]\n",
        "\n",
        "      if next_word == '<\\\\s>':\n",
        "        break\n",
        "\n",
        "      output.append(next_word)\n",
        "    return ' '.join(output)\n",
        "\n",
        "  def predict_next(self, text=None, top=5):\n",
        "    \"\"\" Predicts next word with the ngram model.\n",
        "\n",
        "    Args:\n",
        "      text (string or list[string])\n",
        "\n",
        "    Returns:\n",
        "      possible_next_words (list[namedtuple]):\n",
        "        A list of top few possible next words.\n",
        "    \"\"\"\n",
        "\n",
        "    N = self.n\n",
        "    backup_tokens = ['<s>']*(N-1)\n",
        "    if not text:\n",
        "      tokens = backup_tokens\n",
        "\n",
        "    elif type(text)==str:\n",
        "      tokens = backup_tokens + text.split(' ')\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return \n",
        "\n",
        "    elif type(text) == list:\n",
        "      tokens = backup_tokens + text\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return\n",
        "    else:\n",
        "      print('[Error] the input text must be string or list of string')\n",
        "\n",
        "    possible_next_words = self.model[''.join(tokens)][:top]\n",
        "    possible_next_words = [(word.word, word.prob) for word in possible_next_words]\n",
        "\n",
        "    return possible_next_words\n",
        "\n",
        "  def check_existence(self, tokens):\n",
        "    if not ''.join(tokens) in self.model.keys():\n",
        "      print('[Error] the input text {} not in the vocabulary'.format(tokens))\n",
        "      return False\n",
        "    else:\n",
        "      return True"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_CgC8RKjhd0"
      },
      "source": [
        "## Executions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcJhv21YCE8I"
      },
      "source": [
        "### 1. Build 2-gram/4-gram model by processed dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h17uKROx-FZG"
      },
      "source": [
        "twogram = Ngram_model(documents, N=2)\n",
        "fourgram = Ngram_model(documents, N=4)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOV203H4CNg7"
      },
      "source": [
        "### 2. Show the top-5 probable next words and their probability after initial token \\'\\<s\\>\\'  by 2-gram model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoD7wcAJ-PYm",
        "outputId": "ab455f18-892d-40e9-c53e-a369dc31bc16"
      },
      "source": [
        "output = twogram.predict_next(text='<s>', top=5)\n",
        "print('Next word predictions of two gram model:', output)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next word predictions of two gram model: [('i', 0.0586289306023739), ('you', 0.03570552637132607), ('<\\\\s>', 0.031182446113744346), ('the', 0.030971407060111908), ('it', 0.021440427097414488)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WTm0-9Yk-Pt"
      },
      "source": [
        "**Next word predictions of two gram model:** [('i', 0.0586289306023739), ('you', 0.03570552637132607), ('<\\\\s>', 0.031182446113744346), ('the', 0.030971407060111908), ('it', 0.021440427097414488)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C51gJXJO7JBi"
      },
      "source": [
        "### 3. Generate a sentence with 2-gram model and find the POS taggings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBvwvE6q7Vpl",
        "outputId": "7130e034-6b71-43d6-a826-8dd9fb09c97f"
      },
      "source": [
        "output = twogram.predict_sent(max_len=30)\n",
        "print('Generation results of two gram model:', output)\n",
        "nltk.pos_tag(word_tokenize(output))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generation results of two gram model: the modern warfare at amp be confronted by hidden message\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 'DT'),\n",
              " ('modern', 'JJ'),\n",
              " ('warfare', 'NN'),\n",
              " ('at', 'IN'),\n",
              " ('amp', 'NN'),\n",
              " ('be', 'VB'),\n",
              " ('confronted', 'VBN'),\n",
              " ('by', 'IN'),\n",
              " ('hidden', 'JJ'),\n",
              " ('message', 'NN')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQqQam8Tkdm1"
      },
      "source": [
        "**Generation results of two gram model: the modern warfare at amp be confronted by hidden message**\n",
        "[('the', 'DT'),\n",
        " ('modern', 'JJ'),\n",
        " ('warfare', 'NN'),\n",
        " ('at', 'IN'),\n",
        " ('amp', 'NN'),\n",
        " ('be', 'VB'),\n",
        " ('confronted', 'VBN'),\n",
        " ('by', 'IN'),\n",
        " ('hidden', 'JJ'),\n",
        " ('message', 'NN')]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtlOaSlW73wT"
      },
      "source": [
        "### 4. Generate a sentence with 4-gram model and find the POS taggings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ts-Fcd6y74C-",
        "outputId": "30610380-950c-40ed-9948-d673ef99ab9e"
      },
      "source": [
        "output = fourgram.predict_sent(max_len=30)\n",
        "print('Generation results of four gram model: ', output)\n",
        "nltk.pos_tag(word_tokenize(output))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generation results of four gram model:  all garbage sources for entertainment news and pretty much everything else\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('all', 'DT'),\n",
              " ('garbage', 'NN'),\n",
              " ('sources', 'NNS'),\n",
              " ('for', 'IN'),\n",
              " ('entertainment', 'NN'),\n",
              " ('news', 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('pretty', 'RB'),\n",
              " ('much', 'JJ'),\n",
              " ('everything', 'NN'),\n",
              " ('else', 'RB')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEgbE82RknDg"
      },
      "source": [
        "**Generation results of four gram model:  all garbage sources for entertainment news and pretty much everything else**\n",
        "[('all', 'DT'),\n",
        " ('garbage', 'NN'),\n",
        " ('sources', 'NNS'),\n",
        " ('for', 'IN'),\n",
        " ('entertainment', 'NN'),\n",
        " ('news', 'NN'),\n",
        " ('and', 'CC'),\n",
        " ('pretty', 'RB'),\n",
        " ('much', 'JJ'),\n",
        " ('everything', 'NN'),\n",
        " ('else', 'RB')]"
      ]
    }
  ]
}